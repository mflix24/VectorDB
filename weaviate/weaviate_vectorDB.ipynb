{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1MhKPWA10UgcB8BfWOqDqdaa34rHBFCrV","authorship_tag":"ABX9TyPgB5Z+kfO3QRQq2+FSjhLq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Weaviate Vector Database :\n","This is another kind of vector database where we convert our text into numbers and we allow mathematical operations on top of the numbers. This type of database follows the data through online like pinecone."],"metadata":{"id":"ECDMaLZBjQj8"}},{"cell_type":"code","source":["# installing few components through pip\n","!pip install weaviate-client\n","!pip install langchain\n","!pip install openai"],"metadata":{"id":"yHUJKhTMjBQW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"YaP5HgREiXKV","executionInfo":{"status":"ok","timestamp":1710957188833,"user_tz":-360,"elapsed":5582,"user":{"displayName":"Mehedi Hasan","userId":"10409185127046106302"}}},"outputs":[],"source":["# get your own api key from goggle colab\n","from google.colab import userdata\n","OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n","WEAVIATE_API_KEY = userdata.get('WEAVIATE_API_KEY')\n","WEAVIATE_CLUSTER = userdata.get('WEAVIATE_CLUSTER')"]},{"cell_type":"code","source":["# get the key from our defined api key\n","# (followed by upper cell's api key variable)\n","import os\n","os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"],"metadata":{"id":"4FrTYuhslasB","executionInfo":{"status":"ok","timestamp":1710957188834,"user_tz":-360,"elapsed":7,"user":{"displayName":"Mehedi Hasan","userId":"10409185127046106302"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# installing pypdf through pip\n","!pip install pypdf"],"metadata":{"id":"99OJvmbmltl0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating a folder\n","!mkdir pdfs"],"metadata":{"id":"kUge6FSYltR7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# importing PyPDFDirectoryLoader from langchain.document_loader\n","# here again langchain has a component where we call it and load ourdatqset\n","# this is a RAG system\n","# step-1 : loading the dataset\n","from langchain.document_loaders import PyPDFDirectoryLoader\n","loader = PyPDFDirectoryLoader(\"pdfs\")\n","data = loader.load()"],"metadata":{"id":"5LQfrtFiltOf","executionInfo":{"status":"ok","timestamp":1710957200095,"user_tz":-360,"elapsed":1835,"user":{"displayName":"Mehedi Hasan","userId":"10409185127046106302"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Step-02 : Text Splitting into chunks because of the limitations of the tokens. token is nothing but a single word\n","# importing RecursiveCharacterTextSplitter\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","# creating object and passing few parameters\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n","docs = text_splitter.split_documents(data)"],"metadata":{"id":"suUg9uURyOpi","executionInfo":{"status":"ok","timestamp":1710957200095,"user_tz":-360,"elapsed":9,"user":{"displayName":"Mehedi Hasan","userId":"10409185127046106302"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# checking the length of the entire corpus or documents or dataset\n","len(docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5vFOFgss8n8-","executionInfo":{"status":"ok","timestamp":1710957200095,"user_tz":-360,"elapsed":8,"user":{"displayName":"Mehedi Hasan","userId":"10409185127046106302"}},"outputId":"9ca18d10-2204-4843-f42d-e20dec81883f"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["39"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# print all the page content of the whole dataset\n","for i in docs:\n","  print(i.page_content)"],"metadata":{"id":"isFiNjJp8nrn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step-03 : Embeddings of the whole dataset that was just chunked wise\n","# importing OpenAIEmbeddings but we can also import model from huggingface hub.\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","embeddings = OpenAIEmbeddings()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6NT20Sp28npy","executionInfo":{"status":"ok","timestamp":1710957201687,"user_tz":-360,"elapsed":1595,"user":{"displayName":"Mehedi Hasan","userId":"10409185127046106302"}},"outputId":"ed2d0299-ca93-4716-cc70-10ed909bf3b5"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n","  warn_deprecated(\n"]}]},{"cell_type":"code","source":["# we should import OpenAIEmbeddings through these codes\n","# at first update langchain-openai through this commnad\n","# !pip install -U langchain-openai\n","# then import it through this code\n","# from langchain_openai import OpenAIEmbeddings\n","# embeddings = OpenAIEmbeddings()"],"metadata":{"id":"KTA-fByXAKe9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now there is no warnings showing\n","from langchain_openai import OpenAIEmbeddings\n","embeddings = OpenAIEmbeddings()"],"metadata":{"id":"y-0HlEvgAZy-","executionInfo":{"status":"ok","timestamp":1710957538573,"user_tz":-360,"elapsed":492,"user":{"displayName":"Mehedi Hasan","userId":"10409185127046106302"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# printing embeddings\n","print(embeddings)"],"metadata":{"id":"lAj3vpd8BOvk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# embeddings are stored into vector database\n","# and here we are using weaviate vector database"],"metadata":{"id":"frTCrjEOBOr_","executionInfo":{"status":"ok","timestamp":1710959828558,"user_tz":-360,"elapsed":418,"user":{"displayName":"Mehedi Hasan","userId":"10409185127046106302"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Step-04 : data stored into vector database\n","import weaviate\n","from langchain.vectorstores import Weaviate\n","\n","#Connect to weaviate Cluster\n","auth_config = weaviate.auth.AuthApiKey(api_key = WEAVIATE_API_KEY)\n","WEAVIATE_URL = WEAVIATE_CLUSTER\n","\n","client = weaviate.Client(\n","    url = WEAVIATE_URL,\n","    additional_headers = {\"X-OpenAI-Api-key\": OPENAI_API_KEY},\n","    auth_client_secret = auth_config,\n","    startup_period = 10\n",")"],"metadata":{"id":"r3s-MwSyBOqB","executionInfo":{"status":"ok","timestamp":1710959989226,"user_tz":-360,"elapsed":2389,"user":{"displayName":"Mehedi Hasan","userId":"10409185127046106302"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# we will call the is_ready() function through client\n","# True means the model is ready\n","client.is_ready()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"18qkUoJtBOn6","executionInfo":{"status":"ok","timestamp":1710960072974,"user_tz":-360,"elapsed":398,"user":{"displayName":"Mehedi Hasan","userId":"10409185127046106302"}},"outputId":"1e9b6a9f-863c-4e78-ed75-62fd8e88313a"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# define input structure\n","client.schema.delete_all()\n","client.schema.get()\n","schema = {\n","    \"classes\": [\n","        {\n","            \"class\": \"Chatbot\",\n","            \"description\": \"Documents for chatbot\",\n","            \"vectorizer\": \"text2vec-openai\",\n","            \"moduleConfig\": {\"text2vec-openai\": {\"model\": \"ada\", \"type\": \"text\"}},\n","            \"properties\": [\n","                {\n","                    \"dataType\": [\"text\"],\n","                    \"description\": \"The content of the paragraph\",\n","                    \"moduleConfig\": {\n","                        \"text2vec-openai\": {\n","                            \"skip\": False,\n","                            \"vectorizePropertyName\": False,\n","                        }\n","                    },\n","                    \"name\": \"content\",\n","                },\n","            ],\n","        },\n","    ]\n","}\n","\n","client.schema.create(schema)\n","vectorstore = Weaviate(client, \"Chatbot\", \"content\", attributes=[\"source\"])"],"metadata":{"id":"lHRBzbK3Kzzs","executionInfo":{"status":"ok","timestamp":1710960265402,"user_tz":-360,"elapsed":624,"user":{"displayName":"Mehedi Hasan","userId":"10409185127046106302"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# load text into the vectorstore\n","text_meta_pair = [(doc.page_content, doc.metadata) for doc in docs]\n","texts, meta = list(zip(*text_meta_pair))\n","vectorstore.add_texts(texts, meta)"],"metadata":{"id":"mEirnhsjKzws"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# showing the output\n","query_01 = \"what is a transformer?\"\n","\n","# retrieve text related to the query\n","docs = vectorstore.similarity_search(query_01, top_k=1)"],"metadata":{"id":"E_SqlTIFKzuk","executionInfo":{"status":"ok","timestamp":1710960622787,"user_tz":-360,"elapsed":1148,"user":{"displayName":"Mehedi Hasan","userId":"10409185127046106302"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["docs\n","for j in docs:\n","  print(j.page_content)"],"metadata":{"id":"iO8qBbH_Mjpl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"20F3-SGiMomj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Custom ChatBot :"],"metadata":{"id":"KGKLdpdfM-Up"}},{"cell_type":"code","source":["# importing the libraries\n","from langchain.chains.question_answering import load_qa_chain\n","# from langchain.llms import OpenAI\n","from langchain_openai import OpenAI"],"metadata":{"id":"XJxqrVj3NCgq","executionInfo":{"status":"ok","timestamp":1710960795930,"user_tz":-360,"elapsed":419,"user":{"displayName":"Mehedi Hasan","userId":"10409185127046106302"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# define chain\n","chain = load_qa_chain(\n","    OpenAI(),\n","    chain_type=\"stuff\"\n","    )"],"metadata":{"id":"GYxdd9TxNCmc","executionInfo":{"status":"ok","timestamp":1710960798801,"user_tz":-360,"elapsed":415,"user":{"displayName":"Mehedi Hasan","userId":"10409185127046106302"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# create answer\n","chain.run(input_documents=docs, question=query_01)\n","# LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead\n","# chain.invoke(input_documents=docs, question=query_01, input=input)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"id":"nNAemfxCNlow","executionInfo":{"status":"ok","timestamp":1710961211798,"user_tz":-360,"elapsed":2179,"user":{"displayName":"Mehedi Hasan","userId":"10409185127046106302"}},"outputId":"ec7efa0f-59e0-46e8-934d-d08307883240"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' The Transformer is a type of neural sequence transduction model that relies entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. It has been shown to perform well on simple-language question answering and language modeling tasks and outperforms even previously reported ensembles. The model has an encoder-decoder structure, where the encoder maps an input sequence of symbols to a sequence of continuous representations, and the decoder generates an output sequence of symbols one element at a time. The Transformer also plans to explore using attention-based models for tasks involving input and output modalities other than text, such as images, audio, and video.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}]}]}