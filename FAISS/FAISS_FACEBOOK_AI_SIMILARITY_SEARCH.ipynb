{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"15RoXG9-e98ncBP3LsPqADouelNlWQe3C","authorship_tag":"ABX9TyOnsxcvnu82VUICAkbZywGA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["FAISS : The full form of this term is Facebook AI Similarity Search. This is another kind of vector database where we convert and store the texts data into vectors. This is a local vector database system."],"metadata":{"id":"Ya8JhxSqBp5Y"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"d5LBWmR2AMPf"},"outputs":[],"source":["# importing few libraries\n","!pip -q install langchain\n","!pip install pypdf\n","!pip install sentence-transformers==2.2.2\n","!pip install openai\n","!pip install tiktoken\n","!pip install faiss-cpu"]},{"cell_type":"code","source":["# creating a folder through this command\n","!mkdir pdfs"],"metadata":{"id":"mbiYyEU5A1TN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# importing PyPDFDirectoryLoader() class from document_loaders of the langchain\n","from langchain.document_loaders import PyPDFDirectoryLoader"],"metadata":{"id":"espbvZPwA1Vq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step-01 : loading the dataset\n","loader = PyPDFDirectoryLoader(\"pdfs\")\n","data = loader.load()"],"metadata":{"id":"z1njIrGqA1Y9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this is our whole dataset\n","data"],"metadata":{"id":"OKjONDoEA1sU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step-02 : Splitting the text into chunkings\n","# importing RecursiveCharacterTextSplitter() class for chunkings\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","# creating an object of RecursiveCharacterTextSplitter()\n","# and passing few parameters\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n","text_chunks = text_splitter.split_documents(data)"],"metadata":{"id":"EUJhmy0aA18v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install pdfquery\n","# from pdfquery import PDFQuery\n","# pdf = PDFQuery(\"/content/pdfs/yolo.pdf\")\n","# text = pdf.pq(\"LTTextLineHorizontal\").text()\n","# data = text\n","# print(data)"],"metadata":{"id":"tmluPwbtEJJR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking the length of the chunk\n","len(text_chunks)"],"metadata":{"id":"Iot3seT7PBxA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# showing specific chunk through indexing\n","text_chunks[0]\n","# showing the page_content\n","text_chunks[0].page_content\n","# showing the source where the chunk's of the specific index exist.\n","text_chunks[0].metadata"],"metadata":{"id":"yy_WCf8uPBtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# step-03 : Embeddings of the chunks >> for doing this we need a model\n","# for this we will go to the langchain and following it to the embedding\n","# section and we will import HuggingFaceEmbeddings and this HuggingFaceEmbeddings will\n","# import opensourcemodel from HuggingFace Hub. This is the way we embed our text into vectors\n","from langchain.embeddings import HuggingFaceEmbeddings\n","embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"],"metadata":{"id":"qFyG7maMPBrn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now our data is ready to store and we use here the FAISS vector database system\n","from langchain.vectorstores import FAISS\n","vectorstore=FAISS.from_documents(text_chunks, embeddings)\n"],"metadata":{"id":"Yin5mIHbP9sY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now our vector database is ready for explore\n","query_01 = \"What is Generative AI?\""],"metadata":{"id":"daZLoFvuP9qU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calling the similarity_search() function and passing arguments and parameters\n","docs = vectorstore.similarity_search(query_01, k=3)"],"metadata":{"id":"v-cyHaroP9oP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# we have 3 outputs\n","docs[2].page_content"],"metadata":{"id":"gPVrimHpXX7s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking the outout length\n","len(docs)"],"metadata":{"id":"TYJXwbDIX0eR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3 outputs\n","for i in docs:\n","  print(i.page_content)\n"],"metadata":{"id":"9vSQJZfFX7G6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZApwsLHFYKR1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we will use OpenAI API Key and call the OpenAI from large language models of the langchain. langchain is nothing but chaining the components."],"metadata":{"id":"loO1qASyYqdK"}},{"cell_type":"code","source":["# importing from google colab as userdata\n","from google.colab import userdata\n","OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"Pc96f8YTeDQt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# importing os to access the key through our windows\n","# get the key from defined variable\n","import os\n","os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"],"metadata":{"id":"oJN72MKIeENP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# importing OpenAI from langchain.llms\n","from langchain.llms import OpenAI\n","llm=OpenAI()"],"metadata":{"id":"xR3bGnh4eEJ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# when we use OpenAI we just pass the vector database, the whole database\n","# and its kind of question and answering\n","from langchain.chains import RetrievalQA"],"metadata":{"id":"WF3KIKI4eEHv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now our data is ready to store and we use here the FAISS vector database system\n","# from langchain.vectorstores import FAISS\n","# vectorstore=FAISS.from_documents(text_chunks, embeddings)"],"metadata":{"id":"VcWfsWkigIXI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["qa = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\",\n","    retriever=vectorstore.as_retriever()\n","    )\n"],"metadata":{"id":"P8WZsMApf4ym"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the openai model is ready to answer our questions\n","query_02 = \"What is Generative Ai?\""],"metadata":{"id":"lrKN2ZGIgQDX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print the query through run() method\n","print(qa.run(query_02))"],"metadata":{"id":"OG0JgMsPgP2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"W2BJ7KuLgPrn"},"execution_count":null,"outputs":[]}]}